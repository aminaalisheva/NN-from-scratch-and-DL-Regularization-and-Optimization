{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminaalisheva/NN-from-scratch-and-DL-Regularization-and-Optimization/blob/main/NN_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbWa1My9vlWK"
      },
      "source": [
        "# Neural Network from scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnl0C6G7hX9A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Value:\n",
        "    def __init__(self, data, _prev=(), _op='', requires_grad=False, label=''):\n",
        "        self.data = data\n",
        "        self._prev = _prev\n",
        "        self._op = _op\n",
        "        self.label = label\n",
        "        self.grad = 0.0\n",
        "        self.requires_grad = requires_grad\n",
        "        self._backward = lambda: None\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+', self.requires_grad or other.requires_grad)\n",
        "\n",
        "        def _backward():\n",
        "            if self.requires_grad:\n",
        "                self.grad += out.grad\n",
        "            if other.requires_grad:\n",
        "                other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*', self.requires_grad or other.requires_grad)\n",
        "\n",
        "        def _backward():\n",
        "            if self.requires_grad:\n",
        "                self.grad += other.data * out.grad\n",
        "            if other.requires_grad:\n",
        "                other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, power):\n",
        "        assert isinstance(power, (int, float)), \"Only supports scalar powers.\"\n",
        "        out = Value(self.data**power, (self,), f\"**{power}\", self.requires_grad)\n",
        "\n",
        "        def _backward():\n",
        "            if self.requires_grad:\n",
        "                self.grad += (power * self.data**(power - 1)) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def exp(self):\n",
        "        out = Value(np.exp(self.data), (self,), \"exp\", self.requires_grad)\n",
        "\n",
        "        def _backward():\n",
        "            if self.requires_grad:\n",
        "                self.grad += out.data * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def log(self):\n",
        "        if self.data <= 0:\n",
        "            raise ValueError(\"Logarithm is not defined for non-positive values.\")\n",
        "        out = Value(np.log(self.data), (self,), 'log', self.requires_grad)\n",
        "\n",
        "        def _backward():\n",
        "            if self.requires_grad:\n",
        "                self.grad += (1.0 / self.data) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return self + (Value(-1) * other)\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data / other.data, (self, other), '/', self.requires_grad or other.requires_grad)\n",
        "\n",
        "        def _backward():\n",
        "            if self.requires_grad:\n",
        "                self.grad += (1.0 / other.data) * out.grad\n",
        "            if other.requires_grad:\n",
        "                other.grad += (-self.data / (other.data ** 2)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def sigmoid(self):\n",
        "        s = 1.0 / (1 + np.exp(-self.data))\n",
        "        out = Value(s, (self,), 'sigmoid', self.requires_grad)\n",
        "\n",
        "        def _backward():\n",
        "            if self.requires_grad:\n",
        "                self.grad += s * (1.0 - s) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "        topo = self.build_topo()\n",
        "        self.grad = 1.0\n",
        "\n",
        "        for node in reversed(topo):\n",
        "            node._backward()\n",
        "\n",
        "    def build_topo(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "\n",
        "        def _build_topo(node):\n",
        "            if node not in visited:\n",
        "                visited.add(node)\n",
        "                for child in node._prev:\n",
        "                    _build_topo(child)\n",
        "                topo.append(node)\n",
        "\n",
        "        _build_topo(self)\n",
        "        return topo\n",
        "\n",
        "    def optimize(self, learning_rate=0.01):\n",
        "        for node in self.build_topo():\n",
        "            if node.requires_grad:\n",
        "                node.data -= learning_rate * node.grad\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for node in self.build_topo():\n",
        "            node.grad = 0.0\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Value({self.data})'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NatCqXKZABkT"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, N, activation=True):\n",
        "    self.W = [Value(np.random.uniform(-1, 1), label=f'w{i}', requires_grad=True) for i in range(N)]\n",
        "    self.b = Value(0, label='b', requires_grad=True)\n",
        "    self.activation = activation\n",
        "\n",
        "  def __call__(self, X):\n",
        "    out = self.b\n",
        "    for x, w in zip(X, self.W):\n",
        "        out = out + (x * w)\n",
        "    return out.sigmoid() if self.activation else out\n",
        "\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self, N, count, activation=True):\n",
        "    self.neurons = [Neuron(N, activation) for _ in range(count)]\n",
        "\n",
        "  def __call__(self, X):\n",
        "    return [n(X) for n in self.neurons]\n",
        "\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self, N, counts):\n",
        "    dims = [N] + counts\n",
        "    self.layers = [\n",
        "      Layer(dims[i], dims[i + 1], activation=(i < len(dims) - 2)) # Last layer has activation=False\n",
        "      for i in range(len(dims) - 1)\n",
        "    ]\n",
        "\n",
        "  def __call__(self, X):\n",
        "    out = X\n",
        "    for layer in self.layers:\n",
        "      out = layer(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEkDFgRd0GxH",
        "outputId": "0ac6580e-7836-4d2f-c247-86c1a9058a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: (120, 4), (120,)\n",
            "Test data shape: (30, 4), (30,)\n",
            "Input Samples:\n",
            " [[6.3 2.8 5.1 1.5]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [4.8 3.4 1.9 0.2]]\n",
            "Labels:\n",
            " [2 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X = iris.data  # 50x3 4-dimensional samples\n",
        "y = iris.target # 3 classes (0, 1, 2)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(f'Train data shape: {X_train.shape}, {y_train.shape}')\n",
        "print(f'Test data shape: {X_test.shape}, {y_test.shape}')\n",
        "print(f'Input Samples:\\n {X_train[:5]}')\n",
        "print(f'Labels:\\n {y_train[:5]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI6Rdlms5up2"
      },
      "outputs": [],
      "source": [
        "# converting numpy float arrays into Value lists\n",
        "X_train = [[Value(x) for x in X] for X in X_train]\n",
        "X_test = [[Value(x) for x in X] for X in X_test]\n",
        "y_train = [Value(y) for y in y_train]\n",
        "y_test = [Value(y) for y in y_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-zYdX8LPzxf"
      },
      "source": [
        "# Training Custom MLP Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD_IcT90tK2e"
      },
      "outputs": [],
      "source": [
        "class Classifier:\n",
        "    def __init__(self, layer_sizes=[2, 3, 3]):\n",
        "      self.layer_sizes = layer_sizes\n",
        "      self.nn = None\n",
        "      self.L = Value(0.0)\n",
        "      self.iterations = 0\n",
        "\n",
        "    def one_hot_encode(self, y, num_classes):\n",
        "      return [[Value(1.0) if i == label.data else Value(0.0) for i in range(num_classes)] for label in y]\n",
        "\n",
        "    def softmax(self, logits):\n",
        "      exp_logits = [logit.exp() for logit in logits]\n",
        "      exp_sum = sum(exp_logits, Value(0.0))\n",
        "      return [exp / exp_sum for exp in exp_logits]\n",
        "\n",
        "    def argmax(self, values):\n",
        "      return max(range(len(values)), key=lambda i: values[i].data)\n",
        "\n",
        "    def forward(self, Xs):\n",
        "      raw_outputs = [self.nn(X) for X in Xs]\n",
        "      out = [self.softmax(logit) for logit in raw_outputs]\n",
        "      return out\n",
        "\n",
        "    def predict(self, Xs):\n",
        "      preds = self.forward(Xs)\n",
        "      return [self.argmax(pred) for pred in preds]\n",
        "\n",
        "    def train(self, X_train, y_train, learning_rate=0.01):\n",
        "      self.L.zero_grad()\n",
        "      preds = self.forward(X_train)\n",
        "      y_train_one_hot = self.one_hot_encode(y_train, num_classes=self.layer_sizes[-1])\n",
        "      self.L = self.cross_entropy_loss(y_train_one_hot, preds)\n",
        "      self.L.backward()\n",
        "      self.L.optimize(learning_rate)\n",
        "      print(f'Loss: {self.L.data:.4f}')\n",
        "\n",
        "    def fit(self, X_train, y_train, learning_rate=0.01, num_epochs=50):\n",
        "      if not self.nn:\n",
        "        self.nn = MLP(len(X_train[0]), self.layer_sizes)\n",
        "      for i in range(num_epochs):\n",
        "        print(f'Training epoch {self.iterations + i + 1}')\n",
        "        self.train(X_train, y_train, learning_rate)\n",
        "      self.iterations += num_epochs\n",
        "\n",
        "    def cross_entropy_loss(self, y_true, y_pred):\n",
        "      losses = []\n",
        "      for true, pred in zip(y_true, y_pred):\n",
        "        correct_class_index = self.argmax([t for t in true])\n",
        "        clipped_prob = pred[correct_class_index]\n",
        "        log_prob = clipped_prob.log()\n",
        "        losses.append(log_prob * Value(-1))\n",
        "      total_loss = sum(losses, Value(0.0))\n",
        "      avg_loss = total_loss / Value(len(losses))\n",
        "      return avg_loss\n",
        "\n",
        "    def accuracy_score(self, y_test, preds):\n",
        "      y_test_values = np.array([y.data for y in y_test])\n",
        "      preds_values = np.array(preds)\n",
        "      correct = np.sum(y_test_values == preds_values)\n",
        "      return correct / len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky1M7deDGGSV"
      },
      "outputs": [],
      "source": [
        "model = Classifier([3])\n",
        "# model = Classifier([4, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWZQfsr4JIjC",
        "outputId": "6c500766-7211-476d-d4cc-08032666b8ce",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1\n",
            "Loss: 2.3298\n",
            "Training epoch 2\n",
            "Loss: 1.4518\n",
            "Training epoch 3\n",
            "Loss: 0.7925\n",
            "Training epoch 4\n",
            "Loss: 0.7252\n",
            "Training epoch 5\n",
            "Loss: 0.6637\n",
            "Training epoch 6\n",
            "Loss: 0.6410\n",
            "Training epoch 7\n",
            "Loss: 0.6225\n",
            "Training epoch 8\n",
            "Loss: 0.6078\n",
            "Training epoch 9\n",
            "Loss: 0.5949\n",
            "Training epoch 10\n",
            "Loss: 0.5872\n",
            "Training epoch 11\n",
            "Loss: 0.5796\n",
            "Training epoch 12\n",
            "Loss: 0.5820\n",
            "Training epoch 13\n",
            "Loss: 0.5782\n",
            "Training epoch 14\n",
            "Loss: 0.5951\n",
            "Training epoch 15\n",
            "Loss: 0.5869\n",
            "Training epoch 16\n",
            "Loss: 0.6182\n",
            "Training epoch 17\n",
            "Loss: 0.5926\n",
            "Training epoch 18\n",
            "Loss: 0.6318\n",
            "Training epoch 19\n",
            "Loss: 0.5886\n",
            "Training epoch 20\n",
            "Loss: 0.6310\n",
            "Training epoch 21\n",
            "Loss: 0.5800\n",
            "Training epoch 22\n",
            "Loss: 0.6242\n",
            "Training epoch 23\n",
            "Loss: 0.5707\n",
            "Training epoch 24\n",
            "Loss: 0.6166\n",
            "Training epoch 25\n",
            "Loss: 0.5620\n",
            "Training epoch 26\n",
            "Loss: 0.6091\n",
            "Training epoch 27\n",
            "Loss: 0.5537\n",
            "Training epoch 28\n",
            "Loss: 0.6020\n",
            "Training epoch 29\n",
            "Loss: 0.5458\n",
            "Training epoch 30\n",
            "Loss: 0.5950\n",
            "Training epoch 31\n",
            "Loss: 0.5384\n",
            "Training epoch 32\n",
            "Loss: 0.5883\n",
            "Training epoch 33\n",
            "Loss: 0.5313\n",
            "Training epoch 34\n",
            "Loss: 0.5817\n",
            "Training epoch 35\n",
            "Loss: 0.5245\n",
            "Training epoch 36\n",
            "Loss: 0.5754\n",
            "Training epoch 37\n",
            "Loss: 0.5179\n",
            "Training epoch 38\n",
            "Loss: 0.5691\n",
            "Training epoch 39\n",
            "Loss: 0.5116\n",
            "Training epoch 40\n",
            "Loss: 0.5629\n",
            "Training epoch 41\n",
            "Loss: 0.5054\n",
            "Training epoch 42\n",
            "Loss: 0.5569\n",
            "Training epoch 43\n",
            "Loss: 0.4994\n",
            "Training epoch 44\n",
            "Loss: 0.5509\n",
            "Training epoch 45\n",
            "Loss: 0.4936\n",
            "Training epoch 46\n",
            "Loss: 0.5450\n",
            "Training epoch 47\n",
            "Loss: 0.4879\n",
            "Training epoch 48\n",
            "Loss: 0.5392\n",
            "Training epoch 49\n",
            "Loss: 0.4824\n",
            "Training epoch 50\n",
            "Loss: 0.5335\n",
            "Training epoch 51\n",
            "Loss: 0.4769\n",
            "Training epoch 52\n",
            "Loss: 0.5277\n",
            "Training epoch 53\n",
            "Loss: 0.4716\n",
            "Training epoch 54\n",
            "Loss: 0.5221\n",
            "Training epoch 55\n",
            "Loss: 0.4663\n",
            "Training epoch 56\n",
            "Loss: 0.5164\n",
            "Training epoch 57\n",
            "Loss: 0.4612\n",
            "Training epoch 58\n",
            "Loss: 0.5108\n",
            "Training epoch 59\n",
            "Loss: 0.4561\n",
            "Training epoch 60\n",
            "Loss: 0.5052\n",
            "Training epoch 61\n",
            "Loss: 0.4510\n",
            "Training epoch 62\n",
            "Loss: 0.4997\n",
            "Training epoch 63\n",
            "Loss: 0.4461\n",
            "Training epoch 64\n",
            "Loss: 0.4941\n",
            "Training epoch 65\n",
            "Loss: 0.4412\n",
            "Training epoch 66\n",
            "Loss: 0.4886\n",
            "Training epoch 67\n",
            "Loss: 0.4363\n",
            "Training epoch 68\n",
            "Loss: 0.4831\n",
            "Training epoch 69\n",
            "Loss: 0.4315\n",
            "Training epoch 70\n",
            "Loss: 0.4776\n",
            "Training epoch 71\n",
            "Loss: 0.4268\n",
            "Training epoch 72\n",
            "Loss: 0.4722\n",
            "Training epoch 73\n",
            "Loss: 0.4221\n",
            "Training epoch 74\n",
            "Loss: 0.4667\n",
            "Training epoch 75\n",
            "Loss: 0.4174\n",
            "Training epoch 76\n",
            "Loss: 0.4612\n",
            "Training epoch 77\n",
            "Loss: 0.4128\n",
            "Training epoch 78\n",
            "Loss: 0.4558\n",
            "Training epoch 79\n",
            "Loss: 0.4082\n",
            "Training epoch 80\n",
            "Loss: 0.4504\n",
            "Training epoch 81\n",
            "Loss: 0.4036\n",
            "Training epoch 82\n",
            "Loss: 0.4449\n",
            "Training epoch 83\n",
            "Loss: 0.3991\n",
            "Training epoch 84\n",
            "Loss: 0.4395\n",
            "Training epoch 85\n",
            "Loss: 0.3945\n",
            "Training epoch 86\n",
            "Loss: 0.4341\n",
            "Training epoch 87\n",
            "Loss: 0.3901\n",
            "Training epoch 88\n",
            "Loss: 0.4287\n",
            "Training epoch 89\n",
            "Loss: 0.3856\n",
            "Training epoch 90\n",
            "Loss: 0.4233\n",
            "Training epoch 91\n",
            "Loss: 0.3812\n",
            "Training epoch 92\n",
            "Loss: 0.4179\n",
            "Training epoch 93\n",
            "Loss: 0.3767\n",
            "Training epoch 94\n",
            "Loss: 0.4125\n",
            "Training epoch 95\n",
            "Loss: 0.3723\n",
            "Training epoch 96\n",
            "Loss: 0.4071\n",
            "Training epoch 97\n",
            "Loss: 0.3679\n",
            "Training epoch 98\n",
            "Loss: 0.4017\n",
            "Training epoch 99\n",
            "Loss: 0.3636\n",
            "Training epoch 100\n",
            "Loss: 0.3964\n",
            "Training epoch 101\n",
            "Loss: 0.3592\n",
            "Training epoch 102\n",
            "Loss: 0.3910\n",
            "Training epoch 103\n",
            "Loss: 0.3549\n",
            "Training epoch 104\n",
            "Loss: 0.3857\n",
            "Training epoch 105\n",
            "Loss: 0.3506\n",
            "Training epoch 106\n",
            "Loss: 0.3803\n",
            "Training epoch 107\n",
            "Loss: 0.3463\n",
            "Training epoch 108\n",
            "Loss: 0.3750\n",
            "Training epoch 109\n",
            "Loss: 0.3420\n",
            "Training epoch 110\n",
            "Loss: 0.3697\n",
            "Training epoch 111\n",
            "Loss: 0.3377\n",
            "Training epoch 112\n",
            "Loss: 0.3643\n",
            "Training epoch 113\n",
            "Loss: 0.3334\n",
            "Training epoch 114\n",
            "Loss: 0.3590\n",
            "Training epoch 115\n",
            "Loss: 0.3292\n",
            "Training epoch 116\n",
            "Loss: 0.3537\n",
            "Training epoch 117\n",
            "Loss: 0.3249\n",
            "Training epoch 118\n",
            "Loss: 0.3485\n",
            "Training epoch 119\n",
            "Loss: 0.3207\n",
            "Training epoch 120\n",
            "Loss: 0.3432\n",
            "Training epoch 121\n",
            "Loss: 0.3165\n",
            "Training epoch 122\n",
            "Loss: 0.3380\n",
            "Training epoch 123\n",
            "Loss: 0.3123\n",
            "Training epoch 124\n",
            "Loss: 0.3327\n",
            "Training epoch 125\n",
            "Loss: 0.3081\n",
            "Training epoch 126\n",
            "Loss: 0.3275\n",
            "Training epoch 127\n",
            "Loss: 0.3039\n",
            "Training epoch 128\n",
            "Loss: 0.3223\n",
            "Training epoch 129\n",
            "Loss: 0.2997\n",
            "Training epoch 130\n",
            "Loss: 0.3172\n",
            "Training epoch 131\n",
            "Loss: 0.2955\n",
            "Training epoch 132\n",
            "Loss: 0.3120\n",
            "Training epoch 133\n",
            "Loss: 0.2914\n",
            "Training epoch 134\n",
            "Loss: 0.3069\n",
            "Training epoch 135\n",
            "Loss: 0.2872\n",
            "Training epoch 136\n",
            "Loss: 0.3018\n",
            "Training epoch 137\n",
            "Loss: 0.2831\n",
            "Training epoch 138\n",
            "Loss: 0.2968\n",
            "Training epoch 139\n",
            "Loss: 0.2790\n",
            "Training epoch 140\n",
            "Loss: 0.2917\n",
            "Training epoch 141\n",
            "Loss: 0.2749\n",
            "Training epoch 142\n",
            "Loss: 0.2867\n",
            "Training epoch 143\n",
            "Loss: 0.2708\n",
            "Training epoch 144\n",
            "Loss: 0.2818\n",
            "Training epoch 145\n",
            "Loss: 0.2668\n",
            "Training epoch 146\n",
            "Loss: 0.2769\n",
            "Training epoch 147\n",
            "Loss: 0.2627\n",
            "Training epoch 148\n",
            "Loss: 0.2720\n",
            "Training epoch 149\n",
            "Loss: 0.2587\n",
            "Training epoch 150\n",
            "Loss: 0.2672\n",
            "Training epoch 151\n",
            "Loss: 0.2547\n",
            "Training epoch 152\n",
            "Loss: 0.2624\n",
            "Training epoch 153\n",
            "Loss: 0.2508\n",
            "Training epoch 154\n",
            "Loss: 0.2577\n",
            "Training epoch 155\n",
            "Loss: 0.2469\n",
            "Training epoch 156\n",
            "Loss: 0.2530\n",
            "Training epoch 157\n",
            "Loss: 0.2430\n",
            "Training epoch 158\n",
            "Loss: 0.2485\n",
            "Training epoch 159\n",
            "Loss: 0.2391\n",
            "Training epoch 160\n",
            "Loss: 0.2439\n",
            "Training epoch 161\n",
            "Loss: 0.2353\n",
            "Training epoch 162\n",
            "Loss: 0.2395\n",
            "Training epoch 163\n",
            "Loss: 0.2315\n",
            "Training epoch 164\n",
            "Loss: 0.2351\n",
            "Training epoch 165\n",
            "Loss: 0.2278\n",
            "Training epoch 166\n",
            "Loss: 0.2308\n",
            "Training epoch 167\n",
            "Loss: 0.2242\n",
            "Training epoch 168\n",
            "Loss: 0.2266\n",
            "Training epoch 169\n",
            "Loss: 0.2206\n",
            "Training epoch 170\n",
            "Loss: 0.2226\n",
            "Training epoch 171\n",
            "Loss: 0.2170\n",
            "Training epoch 172\n",
            "Loss: 0.2186\n",
            "Training epoch 173\n",
            "Loss: 0.2136\n",
            "Training epoch 174\n",
            "Loss: 0.2147\n",
            "Training epoch 175\n",
            "Loss: 0.2102\n",
            "Training epoch 176\n",
            "Loss: 0.2110\n",
            "Training epoch 177\n",
            "Loss: 0.2069\n",
            "Training epoch 178\n",
            "Loss: 0.2074\n",
            "Training epoch 179\n",
            "Loss: 0.2037\n",
            "Training epoch 180\n",
            "Loss: 0.2039\n",
            "Training epoch 181\n",
            "Loss: 0.2007\n",
            "Training epoch 182\n",
            "Loss: 0.2006\n",
            "Training epoch 183\n",
            "Loss: 0.1977\n",
            "Training epoch 184\n",
            "Loss: 0.1974\n",
            "Training epoch 185\n",
            "Loss: 0.1948\n",
            "Training epoch 186\n",
            "Loss: 0.1944\n",
            "Training epoch 187\n",
            "Loss: 0.1921\n",
            "Training epoch 188\n",
            "Loss: 0.1915\n",
            "Training epoch 189\n",
            "Loss: 0.1895\n",
            "Training epoch 190\n",
            "Loss: 0.1888\n",
            "Training epoch 191\n",
            "Loss: 0.1871\n",
            "Training epoch 192\n",
            "Loss: 0.1863\n",
            "Training epoch 193\n",
            "Loss: 0.1847\n",
            "Training epoch 194\n",
            "Loss: 0.1839\n",
            "Training epoch 195\n",
            "Loss: 0.1825\n",
            "Training epoch 196\n",
            "Loss: 0.1817\n",
            "Training epoch 197\n",
            "Loss: 0.1805\n",
            "Training epoch 198\n",
            "Loss: 0.1797\n",
            "Training epoch 199\n",
            "Loss: 0.1786\n",
            "Training epoch 200\n",
            "Loss: 0.1778\n",
            "Training epoch 201\n",
            "Loss: 0.1767\n",
            "Training epoch 202\n",
            "Loss: 0.1760\n",
            "Training epoch 203\n",
            "Loss: 0.1751\n",
            "Training epoch 204\n",
            "Loss: 0.1743\n",
            "Training epoch 205\n",
            "Loss: 0.1735\n",
            "Training epoch 206\n",
            "Loss: 0.1727\n",
            "Training epoch 207\n",
            "Loss: 0.1720\n",
            "Training epoch 208\n",
            "Loss: 0.1713\n",
            "Training epoch 209\n",
            "Loss: 0.1705\n",
            "Training epoch 210\n",
            "Loss: 0.1699\n",
            "Training epoch 211\n",
            "Loss: 0.1692\n",
            "Training epoch 212\n",
            "Loss: 0.1686\n",
            "Training epoch 213\n",
            "Loss: 0.1679\n",
            "Training epoch 214\n",
            "Loss: 0.1673\n",
            "Training epoch 215\n",
            "Loss: 0.1667\n",
            "Training epoch 216\n",
            "Loss: 0.1661\n",
            "Training epoch 217\n",
            "Loss: 0.1655\n",
            "Training epoch 218\n",
            "Loss: 0.1649\n",
            "Training epoch 219\n",
            "Loss: 0.1643\n",
            "Training epoch 220\n",
            "Loss: 0.1638\n",
            "Training epoch 221\n",
            "Loss: 0.1632\n",
            "Training epoch 222\n",
            "Loss: 0.1627\n",
            "Training epoch 223\n",
            "Loss: 0.1621\n",
            "Training epoch 224\n",
            "Loss: 0.1616\n",
            "Training epoch 225\n",
            "Loss: 0.1610\n",
            "Training epoch 226\n",
            "Loss: 0.1605\n",
            "Training epoch 227\n",
            "Loss: 0.1600\n",
            "Training epoch 228\n",
            "Loss: 0.1594\n",
            "Training epoch 229\n",
            "Loss: 0.1589\n",
            "Training epoch 230\n",
            "Loss: 0.1584\n",
            "Training epoch 231\n",
            "Loss: 0.1579\n",
            "Training epoch 232\n",
            "Loss: 0.1574\n",
            "Training epoch 233\n",
            "Loss: 0.1569\n",
            "Training epoch 234\n",
            "Loss: 0.1563\n",
            "Training epoch 235\n",
            "Loss: 0.1558\n",
            "Training epoch 236\n",
            "Loss: 0.1553\n",
            "Training epoch 237\n",
            "Loss: 0.1548\n",
            "Training epoch 238\n",
            "Loss: 0.1544\n",
            "Training epoch 239\n",
            "Loss: 0.1539\n",
            "Training epoch 240\n",
            "Loss: 0.1534\n",
            "Training epoch 241\n",
            "Loss: 0.1529\n",
            "Training epoch 242\n",
            "Loss: 0.1524\n",
            "Training epoch 243\n",
            "Loss: 0.1519\n",
            "Training epoch 244\n",
            "Loss: 0.1514\n",
            "Training epoch 245\n",
            "Loss: 0.1510\n",
            "Training epoch 246\n",
            "Loss: 0.1505\n",
            "Training epoch 247\n",
            "Loss: 0.1500\n",
            "Training epoch 248\n",
            "Loss: 0.1496\n",
            "Training epoch 249\n",
            "Loss: 0.1491\n",
            "Training epoch 250\n",
            "Loss: 0.1486\n",
            "Training epoch 251\n",
            "Loss: 0.1482\n",
            "Training epoch 252\n",
            "Loss: 0.1477\n",
            "Training epoch 253\n",
            "Loss: 0.1472\n",
            "Training epoch 254\n",
            "Loss: 0.1468\n",
            "Training epoch 255\n",
            "Loss: 0.1463\n",
            "Training epoch 256\n",
            "Loss: 0.1459\n",
            "Training epoch 257\n",
            "Loss: 0.1454\n",
            "Training epoch 258\n",
            "Loss: 0.1450\n",
            "Training epoch 259\n",
            "Loss: 0.1446\n",
            "Training epoch 260\n",
            "Loss: 0.1441\n",
            "Training epoch 261\n",
            "Loss: 0.1437\n",
            "Training epoch 262\n",
            "Loss: 0.1432\n",
            "Training epoch 263\n",
            "Loss: 0.1428\n",
            "Training epoch 264\n",
            "Loss: 0.1424\n",
            "Training epoch 265\n",
            "Loss: 0.1420\n",
            "Training epoch 266\n",
            "Loss: 0.1415\n",
            "Training epoch 267\n",
            "Loss: 0.1411\n",
            "Training epoch 268\n",
            "Loss: 0.1407\n",
            "Training epoch 269\n",
            "Loss: 0.1403\n",
            "Training epoch 270\n",
            "Loss: 0.1398\n",
            "Training epoch 271\n",
            "Loss: 0.1394\n",
            "Training epoch 272\n",
            "Loss: 0.1390\n",
            "Training epoch 273\n",
            "Loss: 0.1386\n",
            "Training epoch 274\n",
            "Loss: 0.1382\n",
            "Training epoch 275\n",
            "Loss: 0.1378\n",
            "Training epoch 276\n",
            "Loss: 0.1374\n",
            "Training epoch 277\n",
            "Loss: 0.1370\n",
            "Training epoch 278\n",
            "Loss: 0.1366\n",
            "Training epoch 279\n",
            "Loss: 0.1362\n",
            "Training epoch 280\n",
            "Loss: 0.1358\n",
            "Training epoch 281\n",
            "Loss: 0.1354\n",
            "Training epoch 282\n",
            "Loss: 0.1350\n",
            "Training epoch 283\n",
            "Loss: 0.1346\n",
            "Training epoch 284\n",
            "Loss: 0.1342\n",
            "Training epoch 285\n",
            "Loss: 0.1338\n",
            "Training epoch 286\n",
            "Loss: 0.1334\n",
            "Training epoch 287\n",
            "Loss: 0.1330\n",
            "Training epoch 288\n",
            "Loss: 0.1327\n",
            "Training epoch 289\n",
            "Loss: 0.1323\n",
            "Training epoch 290\n",
            "Loss: 0.1319\n",
            "Training epoch 291\n",
            "Loss: 0.1315\n",
            "Training epoch 292\n",
            "Loss: 0.1311\n",
            "Training epoch 293\n",
            "Loss: 0.1308\n",
            "Training epoch 294\n",
            "Loss: 0.1304\n",
            "Training epoch 295\n",
            "Loss: 0.1300\n",
            "Training epoch 296\n",
            "Loss: 0.1297\n",
            "Training epoch 297\n",
            "Loss: 0.1293\n",
            "Training epoch 298\n",
            "Loss: 0.1289\n",
            "Training epoch 299\n",
            "Loss: 0.1286\n",
            "Training epoch 300\n",
            "Loss: 0.1282\n",
            "Training epoch 301\n",
            "Loss: 0.1278\n",
            "Training epoch 302\n",
            "Loss: 0.1275\n",
            "Training epoch 303\n",
            "Loss: 0.1271\n",
            "Training epoch 304\n",
            "Loss: 0.1268\n",
            "Training epoch 305\n",
            "Loss: 0.1264\n",
            "Training epoch 306\n",
            "Loss: 0.1261\n",
            "Training epoch 307\n",
            "Loss: 0.1257\n",
            "Training epoch 308\n",
            "Loss: 0.1254\n",
            "Training epoch 309\n",
            "Loss: 0.1250\n",
            "Training epoch 310\n",
            "Loss: 0.1247\n",
            "Training epoch 311\n",
            "Loss: 0.1243\n",
            "Training epoch 312\n",
            "Loss: 0.1240\n",
            "Training epoch 313\n",
            "Loss: 0.1237\n",
            "Training epoch 314\n",
            "Loss: 0.1233\n",
            "Training epoch 315\n",
            "Loss: 0.1230\n",
            "Training epoch 316\n",
            "Loss: 0.1226\n",
            "Training epoch 317\n",
            "Loss: 0.1223\n",
            "Training epoch 318\n",
            "Loss: 0.1220\n",
            "Training epoch 319\n",
            "Loss: 0.1216\n",
            "Training epoch 320\n",
            "Loss: 0.1213\n",
            "Training epoch 321\n",
            "Loss: 0.1210\n",
            "Training epoch 322\n",
            "Loss: 0.1207\n",
            "Training epoch 323\n",
            "Loss: 0.1203\n",
            "Training epoch 324\n",
            "Loss: 0.1200\n",
            "Training epoch 325\n",
            "Loss: 0.1197\n",
            "Training epoch 326\n",
            "Loss: 0.1194\n",
            "Training epoch 327\n",
            "Loss: 0.1191\n",
            "Training epoch 328\n",
            "Loss: 0.1187\n",
            "Training epoch 329\n",
            "Loss: 0.1184\n",
            "Training epoch 330\n",
            "Loss: 0.1181\n",
            "Training epoch 331\n",
            "Loss: 0.1178\n",
            "Training epoch 332\n",
            "Loss: 0.1175\n",
            "Training epoch 333\n",
            "Loss: 0.1172\n",
            "Training epoch 334\n",
            "Loss: 0.1169\n",
            "Training epoch 335\n",
            "Loss: 0.1165\n",
            "Training epoch 336\n",
            "Loss: 0.1162\n",
            "Training epoch 337\n",
            "Loss: 0.1159\n",
            "Training epoch 338\n",
            "Loss: 0.1156\n",
            "Training epoch 339\n",
            "Loss: 0.1153\n",
            "Training epoch 340\n",
            "Loss: 0.1150\n",
            "Training epoch 341\n",
            "Loss: 0.1147\n",
            "Training epoch 342\n",
            "Loss: 0.1144\n",
            "Training epoch 343\n",
            "Loss: 0.1141\n",
            "Training epoch 344\n",
            "Loss: 0.1138\n",
            "Training epoch 345\n",
            "Loss: 0.1135\n",
            "Training epoch 346\n",
            "Loss: 0.1132\n",
            "Training epoch 347\n",
            "Loss: 0.1129\n",
            "Training epoch 348\n",
            "Loss: 0.1126\n",
            "Training epoch 349\n",
            "Loss: 0.1124\n",
            "Training epoch 350\n",
            "Loss: 0.1121\n",
            "Training epoch 351\n",
            "Loss: 0.1118\n",
            "Training epoch 352\n",
            "Loss: 0.1115\n",
            "Training epoch 353\n",
            "Loss: 0.1112\n",
            "Training epoch 354\n",
            "Loss: 0.1109\n",
            "Training epoch 355\n",
            "Loss: 0.1106\n",
            "Training epoch 356\n",
            "Loss: 0.1104\n",
            "Training epoch 357\n",
            "Loss: 0.1101\n",
            "Training epoch 358\n",
            "Loss: 0.1098\n",
            "Training epoch 359\n",
            "Loss: 0.1095\n",
            "Training epoch 360\n",
            "Loss: 0.1092\n",
            "Training epoch 361\n",
            "Loss: 0.1090\n",
            "Training epoch 362\n",
            "Loss: 0.1087\n",
            "Training epoch 363\n",
            "Loss: 0.1084\n",
            "Training epoch 364\n",
            "Loss: 0.1081\n",
            "Training epoch 365\n",
            "Loss: 0.1079\n",
            "Training epoch 366\n",
            "Loss: 0.1076\n",
            "Training epoch 367\n",
            "Loss: 0.1073\n",
            "Training epoch 368\n",
            "Loss: 0.1071\n",
            "Training epoch 369\n",
            "Loss: 0.1068\n",
            "Training epoch 370\n",
            "Loss: 0.1065\n",
            "Training epoch 371\n",
            "Loss: 0.1063\n",
            "Training epoch 372\n",
            "Loss: 0.1060\n",
            "Training epoch 373\n",
            "Loss: 0.1057\n",
            "Training epoch 374\n",
            "Loss: 0.1055\n",
            "Training epoch 375\n",
            "Loss: 0.1052\n",
            "Training epoch 376\n",
            "Loss: 0.1049\n",
            "Training epoch 377\n",
            "Loss: 0.1047\n",
            "Training epoch 378\n",
            "Loss: 0.1044\n",
            "Training epoch 379\n",
            "Loss: 0.1042\n",
            "Training epoch 380\n",
            "Loss: 0.1039\n",
            "Training epoch 381\n",
            "Loss: 0.1037\n",
            "Training epoch 382\n",
            "Loss: 0.1034\n",
            "Training epoch 383\n",
            "Loss: 0.1031\n",
            "Training epoch 384\n",
            "Loss: 0.1029\n",
            "Training epoch 385\n",
            "Loss: 0.1026\n",
            "Training epoch 386\n",
            "Loss: 0.1024\n",
            "Training epoch 387\n",
            "Loss: 0.1021\n",
            "Training epoch 388\n",
            "Loss: 0.1019\n",
            "Training epoch 389\n",
            "Loss: 0.1016\n",
            "Training epoch 390\n",
            "Loss: 0.1014\n",
            "Training epoch 391\n",
            "Loss: 0.1012\n",
            "Training epoch 392\n",
            "Loss: 0.1009\n",
            "Training epoch 393\n",
            "Loss: 0.1007\n",
            "Training epoch 394\n",
            "Loss: 0.1004\n",
            "Training epoch 395\n",
            "Loss: 0.1002\n",
            "Training epoch 396\n",
            "Loss: 0.0999\n",
            "Training epoch 397\n",
            "Loss: 0.0997\n",
            "Training epoch 398\n",
            "Loss: 0.0995\n",
            "Training epoch 399\n",
            "Loss: 0.0992\n",
            "Training epoch 400\n",
            "Loss: 0.0990\n",
            "Training epoch 401\n",
            "Loss: 0.0987\n",
            "Training epoch 402\n",
            "Loss: 0.0985\n",
            "Training epoch 403\n",
            "Loss: 0.0983\n",
            "Training epoch 404\n",
            "Loss: 0.0980\n",
            "Training epoch 405\n",
            "Loss: 0.0978\n",
            "Training epoch 406\n",
            "Loss: 0.0976\n",
            "Training epoch 407\n",
            "Loss: 0.0973\n",
            "Training epoch 408\n",
            "Loss: 0.0971\n",
            "Training epoch 409\n",
            "Loss: 0.0969\n",
            "Training epoch 410\n",
            "Loss: 0.0967\n",
            "Training epoch 411\n",
            "Loss: 0.0964\n",
            "Training epoch 412\n",
            "Loss: 0.0962\n",
            "Training epoch 413\n",
            "Loss: 0.0960\n",
            "Training epoch 414\n",
            "Loss: 0.0957\n",
            "Training epoch 415\n",
            "Loss: 0.0955\n",
            "Training epoch 416\n",
            "Loss: 0.0953\n",
            "Training epoch 417\n",
            "Loss: 0.0951\n",
            "Training epoch 418\n",
            "Loss: 0.0949\n",
            "Training epoch 419\n",
            "Loss: 0.0946\n",
            "Training epoch 420\n",
            "Loss: 0.0944\n",
            "Training epoch 421\n",
            "Loss: 0.0942\n",
            "Training epoch 422\n",
            "Loss: 0.0940\n",
            "Training epoch 423\n",
            "Loss: 0.0938\n",
            "Training epoch 424\n",
            "Loss: 0.0935\n",
            "Training epoch 425\n",
            "Loss: 0.0933\n",
            "Training epoch 426\n",
            "Loss: 0.0931\n",
            "Training epoch 427\n",
            "Loss: 0.0929\n",
            "Training epoch 428\n",
            "Loss: 0.0927\n",
            "Training epoch 429\n",
            "Loss: 0.0925\n",
            "Training epoch 430\n",
            "Loss: 0.0923\n",
            "Training epoch 431\n",
            "Loss: 0.0920\n",
            "Training epoch 432\n",
            "Loss: 0.0918\n",
            "Training epoch 433\n",
            "Loss: 0.0916\n",
            "Training epoch 434\n",
            "Loss: 0.0914\n",
            "Training epoch 435\n",
            "Loss: 0.0912\n",
            "Training epoch 436\n",
            "Loss: 0.0910\n",
            "Training epoch 437\n",
            "Loss: 0.0908\n",
            "Training epoch 438\n",
            "Loss: 0.0906\n",
            "Training epoch 439\n",
            "Loss: 0.0904\n",
            "Training epoch 440\n",
            "Loss: 0.0902\n",
            "Training epoch 441\n",
            "Loss: 0.0900\n",
            "Training epoch 442\n",
            "Loss: 0.0898\n",
            "Training epoch 443\n",
            "Loss: 0.0896\n",
            "Training epoch 444\n",
            "Loss: 0.0894\n",
            "Training epoch 445\n",
            "Loss: 0.0892\n",
            "Training epoch 446\n",
            "Loss: 0.0890\n",
            "Training epoch 447\n",
            "Loss: 0.0888\n",
            "Training epoch 448\n",
            "Loss: 0.0886\n",
            "Training epoch 449\n",
            "Loss: 0.0884\n",
            "Training epoch 450\n",
            "Loss: 0.0882\n",
            "Training epoch 451\n",
            "Loss: 0.0880\n",
            "Training epoch 452\n",
            "Loss: 0.0878\n",
            "Training epoch 453\n",
            "Loss: 0.0876\n",
            "Training epoch 454\n",
            "Loss: 0.0874\n",
            "Training epoch 455\n",
            "Loss: 0.0872\n",
            "Training epoch 456\n",
            "Loss: 0.0870\n",
            "Training epoch 457\n",
            "Loss: 0.0868\n",
            "Training epoch 458\n",
            "Loss: 0.0866\n",
            "Training epoch 459\n",
            "Loss: 0.0864\n",
            "Training epoch 460\n",
            "Loss: 0.0862\n",
            "Training epoch 461\n",
            "Loss: 0.0860\n",
            "Training epoch 462\n",
            "Loss: 0.0858\n",
            "Training epoch 463\n",
            "Loss: 0.0856\n",
            "Training epoch 464\n",
            "Loss: 0.0854\n",
            "Training epoch 465\n",
            "Loss: 0.0853\n",
            "Training epoch 466\n",
            "Loss: 0.0851\n",
            "Training epoch 467\n",
            "Loss: 0.0849\n",
            "Training epoch 468\n",
            "Loss: 0.0847\n",
            "Training epoch 469\n",
            "Loss: 0.0845\n",
            "Training epoch 470\n",
            "Loss: 0.0843\n",
            "Training epoch 471\n",
            "Loss: 0.0841\n",
            "Training epoch 472\n",
            "Loss: 0.0840\n",
            "Training epoch 473\n",
            "Loss: 0.0838\n",
            "Training epoch 474\n",
            "Loss: 0.0836\n",
            "Training epoch 475\n",
            "Loss: 0.0834\n",
            "Training epoch 476\n",
            "Loss: 0.0832\n",
            "Training epoch 477\n",
            "Loss: 0.0830\n",
            "Training epoch 478\n",
            "Loss: 0.0829\n",
            "Training epoch 479\n",
            "Loss: 0.0827\n",
            "Training epoch 480\n",
            "Loss: 0.0825\n",
            "Training epoch 481\n",
            "Loss: 0.0823\n",
            "Training epoch 482\n",
            "Loss: 0.0821\n",
            "Training epoch 483\n",
            "Loss: 0.0820\n",
            "Training epoch 484\n",
            "Loss: 0.0818\n",
            "Training epoch 485\n",
            "Loss: 0.0816\n",
            "Training epoch 486\n",
            "Loss: 0.0814\n",
            "Training epoch 487\n",
            "Loss: 0.0813\n",
            "Training epoch 488\n",
            "Loss: 0.0811\n",
            "Training epoch 489\n",
            "Loss: 0.0809\n",
            "Training epoch 490\n",
            "Loss: 0.0807\n",
            "Training epoch 491\n",
            "Loss: 0.0806\n",
            "Training epoch 492\n",
            "Loss: 0.0804\n",
            "Training epoch 493\n",
            "Loss: 0.0802\n",
            "Training epoch 494\n",
            "Loss: 0.0801\n",
            "Training epoch 495\n",
            "Loss: 0.0799\n",
            "Training epoch 496\n",
            "Loss: 0.0797\n",
            "Training epoch 497\n",
            "Loss: 0.0795\n",
            "Training epoch 498\n",
            "Loss: 0.0794\n",
            "Training epoch 499\n",
            "Loss: 0.0792\n",
            "Training epoch 500\n",
            "Loss: 0.0790\n"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, learning_rate=0.1, num_epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A36dX0tTn45H",
        "outputId": "176790fd-cc5c-437d-fbb7-710a11b96036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom MLP classifier accuracy on train Data: 0.97\n"
          ]
        }
      ],
      "source": [
        "preds = model.predict(X_train)\n",
        "print(f'Custom MLP classifier accuracy on train Data: {model.accuracy_score(y_train, preds):.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFg7VTJSfUGG",
        "outputId": "b8fc5fc3-3505-4c63-ea6b-7e8b1bb5f886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom MLP classifier accuracy on test Data: 1.00\n"
          ]
        }
      ],
      "source": [
        "preds = model.predict(X_test)\n",
        "print(f'Custom MLP classifier accuracy on test Data: {model.accuracy_score(y_test, preds):.2f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
